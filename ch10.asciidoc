== Dask with GPUs and Other Special Resources

.A Note for Early Release Readers
****
With Early Release ebooks, you get books in their earliest form&mdash;the authors' raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.

This will be the 10th chapter of the final book. The GitHub repo is available at https://github.com/scalingpythonml/scalingpythonml.

If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at vwilson@oreilly.com.
****

Sometimes the answer to our scaling problem isn't throwing more computers at it; it's throwing different types of resources at it. For example, ten thousand monkies trying to reproduce the works of Shakespeare v.s. one Shakespearefootnote:[Provided Shakespear is still alive. Which he is not.]. While performance varies, some benchmarks have shown up to an 85% improvement in model training times when using GPUs over CPUsfootnote:[https://datamadness.github.io/TensorFlow2-CPU-vs-GPU#:~:text=While%20setting%20up%20the%20GPU,required%20training%20time%20by%2085%25[+++https://datamadness.github.io/TensorFlow2-CPU-vs-GPU#:~:text=While%20setting%20up%20the%20GPU,required%20training%20time%20by%2085%25+++].]. Continuing its modular tradition, the GPU logic of Dask is found in the libraries and ecosystem surrounding it. The libraries can either run on a collection of GPU workers or parallelize work over different GPUs on one host.

Most work we do on the computer is done on the "Central Processing Unit" (CPU). Graphics Processing Units (GPUs) were created for displaying video, but involve doing large amounts of vectorized floating point (e.g. non-integer) operations. With vectorized operations, the same operation is applied in parallel on large sets of data, like a `map`. Tensor-Processing Units are similar to GPUs, except without also being used for graphics.

For our purposes, in Dask, we can think of GPUs and TPUs as specializing in off-loading large vectorized computations, but there are many other kinds of accelerators that exist. While much of this chapter is focused on GPUs, the same general techniques, albeit with different libraries, generally apply to other accelerators. Other kinds of specialized resources include NVMe drives, faster (or larger) RAM, TCP/IP offload, Just-a-Bunch-of-Disks expansion ports, and Intel's OPTAIN memory. Special resources/accelerators can improve everything from network latency to writing large files to disk. What all these share is that Dask has no built-in understanding of these resources, and it's up to you to provide that information to Dask scheduler and also take advantage of it.

This chapter will look at the current state of accelerated analytics on Python and how to use these tools together with Dask. In this chapter, you will learn what kinds of problems are well suited to GPU acceleration, a bit about other kinds of accelerators, and how to apply this to your problems.

[WARNING]
====
Cloud accounts and machines with access to GPUs are especially of interest to less-than-savory folks on the internet due to the relative ease of mining crypto currency. For those used to working with only public data and lax security controls, take this as an opportunity to review your security process and restrict runtime access to only those who need it. Or be prepared for a really large cloud bill.
====

=== Transparent v.s. non-Transparent Accelerators

Accelerators largely break down into two categories: transparent (no code or change required), and non-transparent optimizers. If an accelerator is transparent or not largely depends on if someone below us in the stack has made it transparent to us.

TCP/IP offloading is generally transparent at the user space level, which means the operating system takes care of it for us. NVMe drives are also generally transparent, generally appearing the same as spinning disks except faster. It is still important to make Dask aware of transparent optimizers, for example, a disk-intensive workload should be scheduled on the machines with faster disks.

The non-transparent accelerators include GPUs, Optane, QAT, and many more. Using these requires changing our code to be able to take advantage of them. Sometimes this can be as simple as swapping in a different library, but not always. Many non-transparent accelerators require either copying our data or special formatting to be able to operate. This means that if an operation is relatively fast, moving to an optimizer could make it slower.

=== Understanding if GPUs or TPUs can help

Not every problem is a good fit for GPU acceleration. GPUs are especially good at performing the same calculation on a large number of data points at the same time. If a problem is well suited to vectorized computation then there is a good chance that GPUs may be well suited to it.

Some common problems that benefit from GPU acceleration include:

* Machine learning
* Linear algebra
* Physics simulations
* Graphics (no surprise here)

GPUs are not well suited to branch-heavy non-vectorized workflows or workflows where the cost of copying the data is similar to or higher than the cost of the computation.

=== Making Dask Resource Aware

If you have decided that your problem is well suited to a specialized resource, the next step is to make the scheduler aware of which machines, and processes, have the resource.footnote:[https://distributed.dask.org/en/latest/resources.html[+++https://distributed.dask.org/en/latest/resources.html+++]] You can do this by adding either environment variable or command line flag to the worker launch (e.g. `--resources "GPU=2"` or `DASK_DISTRIBUTED__WORKER__RESOURCES__GPU=2`).

For NVIDIA users the `dask-cuda` package can launch one worker per GPU, pining the GPU and thread together for performance. For example, on our Kubernetes cluster with GPU resources we configure the workers to use the `dask-cuda-worker` launcher as shown in <<ex_dask_cuda_k8s>>.

[[ex_dask_cuda_k8s]]
.Use the dask-cuda-worker package in the Dask Kubernetes template
====
[source, python]
----
include::./examples/dask/DaskKubeGPUs.py[tags=worker_template_with_gpu]
----
====

Here you see we still add the `--resources` flag so that in a mixed environment we can select just the GPU workers.

If your using Dask to schedule work on multiple GPUs on a single computer, e.g. Dask local mode with CUDA, the same `dask-cuda` package provides a `LocalCUDACluster`, as with `dask-cuda-worker` you still need to add the resource tag manually as shown in <<ex_dask_cuda_local>> but it launches the correct workers and pins them to threads.

[[ex_dask_cuda_local]]
.Local CUDACluster with resource tagging
====
[source, python]
----
include::./examples/dask/DaskGPUs.py[tags=dask_local_gpu]
----
====

[NOTE]
====
For homogenous clusters it may seem tempting to avoid labeling these resources, but unless you will always have a 1:1 mapping of worker process/thread to the accelerator (or the accelerator can be used by all workers at the same time) it is still beneficial to label these resources. This is important for non-shareable (or difficult to share) resources like GPUs/TPUs since Dask might schedule two tasks trying to access the GPU. but for shareable resources like NVMe drives, or TCP/IP off-loading, if it's present on every node in the cluster and will always be you can probably skip it.
====

It's important to note that Dask does not manage custom resources (including GPUs). If another process uses all of the GPU cores without asking Dask there is no protection for this. In some ways, this is reminiscent of early computing, where we had "cooperative" multi-tasking, we depend on our neighbors being well-behaved.

[WARNING]
====
Dask depends on well-behaved Python code, which does not use resources it has not asked for and releases the resources when finished. This most commonly happens with memory leaks (both accelerated and not) commonly with specialized libraries like CUDA which allocate memory outside of Python. These libraries often have special steps you need to call when you are done with the task you've asked to make the resources available for others.
====

=== Installing the libraries

Now that Dask is aware of the special resources on your cluster, it's time to make sure that your code can take advantage of them. Often, but not always, these accelerators will require some kind of special library installed, which may involve long compile times. When possible, installing the acceleration libraries from conda and pre-installing on the workers (in the container or on the host) when possible can help minimize this overhead.

For Kubernetes (or other Docker container users) you can do this by making a custom container with the accelerator libraries pre-installed as in <<preinstall_gpu_docker>>.

[[preinstall_gpu_docker]]
.Pre-Install cudf
====
[source, python]
----
include::./examples/dask/preinstall_numba/Dockerfile[]
----
====

Then to build this we run the following script:

.Build Custom
====
[source, bash]
----
include::./examples/dask/preinstall_numba/build.sh[]
----
====

=== Using Custom Resources inside your Dask Tasks

It is important that you make sure your tasks that need accelerators run on worker processes with the accelerator available. You can ask for special resources when scheduling tasks with Dask, either explicitly in `client.submit` as in <<ex_submit_gpu>> or by adding an annotation to your existing code <<ex_annotate_gpu>>.

[[ex_submit_gpu]]
.Submit a task asking for a GPU
====
[source, python]
----
include::./examples/dask/DaskGPUs.py[tags=ex_submit_gpu]
----
====

[[ex_annotate_gpu]]
.Annotate a group of operations as needing a GPU
====
[source, python]
----
include::./examples/dask/DaskGPUs.py[tags=ex_annotate_gpu]
----
====

If you move from a cluster with GPU resources to a cluster without, this code will hang indefinitely. The "CPU Fallback" design pattern covered later can mitigate this.

==== Decorators (including numba)

Numba is a popular high-performance JIT (just in time) compilation library for Python, which has also has support for various accelerators. Most JIT code, and many decorator functions, are generally not directly serializable, so attempting to directly numba it with dask.submit does not work <<ex_dask_submit_numba_incorrect>>. Instead, the correct way is to wrap the function, as shown in <<ex_dask_submit_numba_correct>>.

[[ex_dask_submit_numba_incorrect]]
.Decorator Difficulty
====
[source, python]
----
include::./examples/dask/DaskGPUs.py[tags=ex_dask_submit_numba_incorrect]
----
====

[[ex_dask_submit_numba]]
.Decorator Hack
====
[source, python]
----
include::./examples/dask/DaskGPUs.py[tags=ex_dask_submit_numba]
----
====

[NOTE]
====
<<ex_dask_submit_numba_incorrect>> will work in local mode – but not when you go to scale.
====

==== GPUs

Like most tasks in Python, there are many different libraries for working with GPUs. Many of these libraries support NVIDIA's Compute Unified Device Architecture (CUDA) with experimental support for AMD's new "open" HIP/ Radeon Open Compute module(ROCm) interfaces. NVIDIA and CUDA was the first on the scene and has a much larger adoption than AMD's Radeon Open Compute module. So much so that ROCm has a large focus on supporting ports of CUDA software to the ROCm platform.

We won't dive deep into the world of Python GPU libraries, but you may want to check out https://numba.readthedocs.io/en/stable/user/5minguide.html#gpu-targets[+++numba for GPUs+++], https://www.tensorflow.org/guide/gpu[+++tensorflow GPUs support+++], https://pytorch.org/docs/stable/notes/cuda.html[+++and PyTorch's GPU support.+++]

Most of the libraries that have some form of GPU support require compiling large amounts of non-Python code. As such it's often best to install these libraries with conda which frequently has more complete binary packaging allowing you to skip the compile step.

=== GPU Acceleration built on top of Dask

The three main CUDA libraries extending Dask are, "cudf" (previously called dask-cudf), and "BlazingSQL"footnote:[This project may be end-of-life, there has not been a commit for an extended period of time and the website is just a hard-hat like those 1990s geocities websites.], and "cuML." Currently these libraries are focused on NVidia GPUs.

[NOTE]
====
Dask does not currently have any libraries powering integrations with OpenCL or HIP. This does not preclude you in any way from using GPUs with libraries that support them, like Tensorflow, as previously illustrated.
====

==== cuDF

https://docs.rapids.ai/api/cudf[+++cuDF+++] is a GPU-accelerated version of Dask's DataFrame library. Some https://arshovon.com/blog/cudf-vs-df/[+++benchmarking shows performance speedsup of 7x~50x+++]. Not all DataFrame operations will have this same speed up. For example, if you are operating row-by-row instead of in vectorized type operations, you may experience slower performance when using cuDF over Dask's DataFrame library. cuDF supports most of the common data types you are likely to use, but not all.

[NOTE]
====
Under the hood cuDF frequently delegates work to the cuPY library, but since it is created by NVIDIA employees and their focus is on supporting NVIDIA hardware, cuDF does not have direct support for ROCm.
====

==== BlazingSQL

BlazingSQL uses GPU acceleration to provide super-fast SQL queries. Blazing SQL operates on top of cuDFs.

[NOTE]
====
While BlazingSQL is a wonderful tool, much of it's documentation is broken. For example, at the time of this writing none of the examples linked in the main README resolve correctly and the documentation site is entirely off-line.
====

==== cuML

Another GPU accelerated library for "streaming on GPUs" is "cuStreamz" which is basically a combination of Dask streaming and cuDF, which we cover more on in <<appa_streaming>>.

=== Freeing Accelerator Resources

Allocating memory on GPUs tend to be slow, so many libraries hold on to these resources. In most situations, if the Python VM exits the resources will be cleared up. An option of last resort is to bounce all of the workers using `client.restart`. When possible, you will be best served by manually managing resources – which is library dependent. For example, cuPY users – you can free the blocks once used by calling `free_all_blocks()`, https://docs.cupy.dev/en/stable/user_guide/memory.html[+++as per the memory management documentation.+++]

=== Design Patterns: CPU Fallback

CPU Fallback refers to attempting to use an accelerator, like GPU or TPU, and "falling-back" to the "regular" CPU code path if the accelerator is unavailable. In most cases, this is a good design pattern to follow, as accelerators (like GPUs) can be expensive and may not always be available. However, in some cases, the performance difference between CPU and GPU performance is so large, that falling back to CPU is unlikely to be able to succeed in a practical amount of time – this occurs most often with deep learning algorithms.

Object-oriented and duck-typing are somewhat well suited to this, since, provided that two classes implement the same parts of the interface you are using, you can swap them around. However, much like swapping in Dask DataFrames for Pandas DataFrames, it is imperfect, especially when it comes to performance.

[WARNING]
====
In a better world, we could submit a task requesting GPU resources, and if that does not get scheduled, we could switch back to CPU-only resources. Unfortunately, Dask's resources scheduling is closer to "best effort"footnote:[This is not as documented, https://distributed.dask.org/en/stable/resources.html[+++https://distributed.dask.org/en/stable/resources.html+++] so may change in the future.], so we may be scheduled on nodes without the resources we request.
====

=== Conclusion

Specialized accelerators, like GPUs, can make large differences in your workflows. Picking the right accelerator for your workflow is important, and some workflows are not well suited to acceleration. Dask does not automate the usage of any accelerators but there are various libraries that you can use for GPU computation. Many of these libraries were not created with the idea of shared computation in mind, so it's important to be on the lookout for accidental resource leaks, especially since GPU resources tend to be more expensive.
